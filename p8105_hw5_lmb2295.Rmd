---
title: "Homework 5"
author: "Laya Buchanan"

date: 2020-11-05
output: github_document
---

This is my submission for the fifth homework assignment for P8105.  

```{r message = FALSE, echo = FALSE}
library(tidyverse)
library(broom)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Read in the data.

```{r}
homicide_df = 
  read_csv("homicide_data/homicide-data.csv") %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved",
    )
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa_AL")
```


Let's look at this a bit

```{r}
aggregate_df = 
  homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )
```

Can I do a prop test for a single city?

```{r}
prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved), 
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)) %>% 
  broom::tidy()
```

Try to iterate ........

```{r}
results_df = 
  aggregate_df %>% 
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
```



```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```




# Problem 2: Longitudinal study data

First, I created a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time from the original folder of .csv files.

```{r}
control_exp_df = 
  map_df(list.files(path = "./data", full.names = TRUE), read.csv) %>% 
  mutate(file_name = list.files(path = "./data")) %>% 
  relocate(file_name, .before = week_1) %>% 
  separate(file_name, into = c("arm", "id"), sep = "_") %>% 
  separate(id, into = c("id", "delete"), sep = "\\.") %>% 
  select(-delete) %>% 
  mutate(arm = recode(arm, 'con' = "control")) %>% 
  mutate(arm = recode(arm, 'exp' = "experimental"))
  
```

Then, I made a spaghetti plot showing observations on each subject over time by week.

```{r}
control_exp_df %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "value"
    ) %>% 
  unite(group, arm:id, sep = '', remove = FALSE) %>% 
  ggplot(aes(x = week, y = value, group = group, color = arm)) + 
  geom_line() + 
  labs(
    title = "Measurement for Control vs Experimental Arm Over Time",
    x = "Week",
    y = "Value") +
   theme(legend.position = "right")
```

While measurements for the variable of interests were comparable at baseline for the control and experimental groups, values increased over time for the experimental group, but not for the control group. There was considerable inter- and intra-individual variability in both groups.

# Problem 3: Power

First, I create a simulated sample with a normal distribution and create a function with a fixed sample size and error variance.
```{r}
sim_t_test = function(n = 30, mu, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma)
  )
  
    sim_data %>% 
      t.test(
      mu = 0,
      conf.level = 0.95) %>% 
      tidy()
  

}
```

Next, I run 5000 datasets from the model for each μ of 0, 1, 2, 3, 4, 5, 6 and save the estimate of the effect estimate and the p-value in a dataframe

```{r results = "hide"}
output = vector("list", length = 5000)
mu_list = 
  list(
    "mu = 0" = 0,
    "mu = 1" = 1,
    "mu = 2" = 2,
    "mu = 3" = 3,
    "mu = 4" = 4,
    "mu = 5" = 5,
    "mu = 6" = 6
  )

for (i in 1:7) {
  output[[i]] = 
    rerun(5000, sim_t_test(mu = mu_list[[i]]))
bind_rows()
}


sim_results =
  tibble(
  mu = c(0, 1, 2, 3, 4, 5, 6)
) %>% 
  mutate(
    output_lists = map(.x = mu, ~rerun(5000, sim_t_test(mu = .x))), 
estimate_df = map(output_lists, bind_rows
  )
) %>% 
  unnest(estimate_df) %>% 
  select(mu, estimate, p.value)
```

Next, I create a plot showing the proportion of times the null was rejected on the y axis and the true value of μ on the x axis.
```{r}
sim_results %>% 
  mutate(
    reject_null = p.value < 0.05
  ) %>% 
  group_by(mu) %>% 
  count(reject_null) %>% 
  filter(reject_null == TRUE) %>% 
  mutate(
    prop_reject = n/5000
  ) %>% 
  ggplot(aes(x = mu, y = prop_reject)) +
  geom_line() +
  labs(
    title = "Power",
    x = "True Mean",
    y = "% of Times Null Rejected") +
    scale_y_continuous(
    breaks = c(0.25, 0.5000, .75, 1.00), 
    labels = c(25, 5000, 75, 100)) +
  scale_x_continuous(
    breaks = c(0, 1, 2, 3, 4, 5, 6))
  
  
  
```

The power of the test increases as effect size increases, with power reaching 100% or nearly 100% with an effect size of about 4.

Next, I create a  a plot showing the average estimate of μ on the y axis and the true value of μ on the x axis, overlaying it with a plot of the average estimate of μ only in samples for which the null was rejected on the y axis and the true value of μ on the x axis.
```{r}
results_all = 
  sim_results %>% 
  group_by(mu) %>% 
  mutate(
    average = mean(estimate)
  )
  
results_true = 
  sim_results %>% 
    mutate(
    reject_null = p.value < 0.05) %>% 
  filter(reject_null == TRUE) %>% 
  group_by(mu) %>% 
  mutate(
    average = mean(estimate)
  )

  colors = c("All Values" = "blue", "Rejected Null" = "red")
  ggplot(results_all, aes(x = mu, y = average)) +
  geom_line(aes(color = "All Values")) +
  geom_line(data = results_true, aes(x = mu, y = average, color = "Rejected Null")) + 
  labs(
    title = "True vs Estimated Mean",
    x = "True Mean",
    y = "Estimated Mean",
    color = "Legend") +
  scale_x_continuous(
    breaks = c(0, 1, 2, 3, 4, 5, 6)) +
  scale_y_continuous(
    breaks = c(0, 1, 2, 3, 4, 5, 6)) +
    scale_color_manual(values = colors)
```

The sample average of the estimated μ across tests for all tests is approximately equal to the true value of μ. This is not true of the observations which the null is rejected; it is only true of the tests with higher μ. This is because in order to reject the null, the estimated μ must be different from 0 at a 95% level of significance. For a small μ of 1 or 2, only an estimated μ larger (due to random chance) than μ will be far enough from 0 to be significant. So, when examining the tests where to null hypothesis was rejected, the estimated μ is larger than the true μ for the smaller effect sizes.